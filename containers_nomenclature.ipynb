{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c72ce0b6-fb66-445c-8847-88b15605b930",
   "metadata": {},
   "source": [
    "# CONTAINERIZATION. [NOMENCLATURE](https://developers.redhat.com/blog/2018/02/22/container-terminology-practical-introduction)\n",
    "\n",
    "by Scott McCarty (fatherlinux), 2015"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2e045a-dda5-42f6-95ac-c9895d51c472",
   "metadata": {},
   "source": [
    "# [Kernel Space](https://www.linfo.org/kernel_space.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98fa232-1a1f-41dd-9550-8bc64d8494ee",
   "metadata": {},
   "source": [
    "System memory in Linux can be divided into two distinct regions: \n",
    "- kernel space and \n",
    "- user space. \n",
    "\n",
    "**Kernel space** is where the kernel (i.e., the core of the operating system) executes (i.e., runs) and provides its services.\n",
    "\n",
    "Memory consists of RAM (random access memory) cells, whose contents can be accessed (i.e., read and written to) at extremely high speeds but are retained only temporarily (i.e., while in use or, at most, while the power supply remains on). Its purpose is to hold programs and data that are currently in use and thereby serve as a high speed intermediary between the CPU (central processing unit) and the much slower storage, which most commonly consists of one or more hard disk drives (HDDs).\n",
    "\n",
    "**User space** is that set of memory locations in which user processes (i.e., everything other than the kernel) run. A **process** is an executing instance of a program. One of the roles of the kernel is to manage individual user processes within this space and to prevent them from interfering with each other.\n",
    "\n",
    "Kernel space can be accessed by user processes only through the use of **system calls**. System calls are requests in a Unix-like operating system by an active process for a service performed by the kernel, such as input/output (I/O) or process creation. An active process is a process that is currently progressing in the CPU, as contrasted with a process that is waiting for its next turn in the CPU. I/O is any program, operation or device that transfers data to or from a CPU and to or from a peripheral device (such as disk drives, keyboards, mice and printers)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d80274-d9d8-4b9d-9bc2-34d3a6b10604",
   "metadata": {},
   "source": [
    "# [User Space](https://www.linfo.org/user_space.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4651b71b-2c4d-4808-af8a-4590110449f9",
   "metadata": {},
   "source": [
    "**User space** is that portion of system memory in which user processes run. This contrasts with **kernel space**, which is that portion of memory in which the kernel executes and provides its services.\n",
    "\n",
    "The contents of memory, which consists of dedicated RAM (random access memory) VLSI (very large scale integrated circuit) semiconductor chips, can be accessed (i.e., read and written to) at extremely high speeds but are retained only temporarily (i.e., while in use or, at most, while the power supply remains on). This contrasts with storage (e.g., disk drives), which has much slower access speeds but whose contents are retained after the power is turned off and which usually has a far greater capacity.\n",
    "\n",
    "A **process** is an executing (i.e., _running_) **instance** of a program. User processes are instances of all programs _other than the kernel_ (i.e., utility and application programs). When a program is to be run, it is copied from storage into user space so that it can be accessed at high speed by the CPU (central processing unit).\n",
    "\n",
    "The **kernel** is a program that constitutes the central core of a computer operating system. It is not a process, but rather a _controller of processes_, and it has complete control over everything that occurs on the system. This includes managing individual user processes within user space and preventing them from interfering with each other.\n",
    "\n",
    "The division of system memory in Unix-like operating systems into user space and kernel space plays an important role in maintaining system stability and security."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d089492-eedd-4381-8ce8-e1284802de8e",
   "metadata": {},
   "source": [
    "![](./data/images/user-space-vs-kernel-space-simple-user-space.png)\n",
    "\n",
    "[Image source](https://www.redhat.com/en/blog/architecting-containers-part-1-why-understanding-user-space-vs-kernel-space-matters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89825ac3-f4fe-40dd-a5a9-aa00891266d3",
   "metadata": {},
   "source": [
    "![](./data/images/user-space-vs-kernel-space-system-calls-gears.png)\n",
    "\n",
    "[Image source](https://www.redhat.com/en/blog/architecting-containers-part-1-why-understanding-user-space-vs-kernel-space-matters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cca8a7-171e-4815-a194-dcc2f3bfc9bc",
   "metadata": {},
   "source": [
    "Notice that some code lives in user space, and some lives in the kernel:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85675698-b488-4d98-98a4-c35709ea7590",
   "metadata": {},
   "source": [
    "![](./data/images/user-space-vs-kernel-space-basic-system-calls.png)\n",
    "\n",
    "[Image source](https://www.redhat.com/en/blog/architecting-containers-part-1-why-understanding-user-space-vs-kernel-space-matters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d84259b-4455-48a5-8e7c-9fe97f654fc3",
   "metadata": {},
   "source": [
    "## FUSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cc8b87-3a45-4b1d-8c61-7968270c5610",
   "metadata": {},
   "source": [
    "_ChatGPT:_  \n",
    "The container applications like Docker and Podman use FUSE (Filesystem in Userspace) to provide container file systems. FUSE allows developers to implement a file system in user space rather than in the kernel, which provides flexibility and allows for the development of custom file systems for specific needs. This allows container applications to create and manage container file systems without needing to modify the host system's kernel. FUSE has been a key technology in enabling the development and management of containerized applications.\n",
    "\n",
    "_Codio Course:_   \n",
    "**FUSE (Filesystem in Userspace)** allows users to create their own file systems without editing Linux kernel code. These file systems may be mounted by users who do not have root privileges. You can use FUSE to extend the features of the Linux file system without the need to learn Linux file system internals or kernel module programming.\n",
    "\n",
    "The **libfuse** library offers two APIs, the high level API allows you to access files using file names and paths and the low level API uses inodes.\n",
    "\n",
    "The libraries and utilities you need to develop your own file system are available [here](https://github.com/libfuse/).\n",
    "\n",
    "More information about [FUSE](https://www.kernel.org/doc/html/latest/filesystems/fuse.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1e3abb-1514-4b92-8a03-124fac7f79eb",
   "metadata": {},
   "source": [
    "# [Toolchain](https://www.redhat.com/en/blog/architecting-containers-part-3-how-user-space-affects-your-application)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9214dedf-4e12-4a77-a693-dd1ff373c1c1",
   "metadata": {},
   "source": [
    "- a single container user space.\n",
    "\n",
    "Choosing and standardizing on a single container user space (toolchain), across environments will enable developers, systems administrators, and architects to simplify communication between development and operations, speeding up deployments. Building up skills and understanding around a single toolchain simplifies communication (between developers and operations), training, and troubleshooting (...if and when things don’t go as planned)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd080bc-9308-4ae1-a662-2977a1dbede0",
   "metadata": {},
   "source": [
    "# Containers 101"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c0d26a-b9ec-4539-bce1-f0d876f32dba",
   "metadata": {},
   "source": [
    "A **container** is really two different things. Like a normal Linux program, containers really have two states:\n",
    "- rest, and \n",
    "- running. \n",
    "\n",
    "When at **rest**, a container is a **file** (or set of files) that is saved on disk. This is referred to as a **Container Image** or **Container Repository**. \n",
    "\n",
    "When you type the command to start a container, the **Container Engine** unpacks the required files and meta-data, then hands them off to the the Linux **kernel**. Starting a container is very similar to starting a normal Linux **process** and requires making an **API call** to the Linux kernel. This API call typically initiates extra isolation and mounts _a copy_ of the files that were in the container image. Once running, **Containers** are just a **Linux process**. \n",
    "\n",
    "The process for starting containers, as well as the image format on disk, are defined and governed by standards.\n",
    "\n",
    "There are several competing Container Image formats ([Docker](https://github.com/docker/docker/blob/master/image/spec/v1.md), [Appc](https://www.redhat.com/en/technologies/cloud-computing/openshift), [LXD](https://ubuntu.com/blog/lxd-2-0-image-management-512)), but the industry is moving forward with a standard governed under the [**Open Container Initiative**](https://opencontainers.org/faq/) - sometimes referred to simply as **Open Containers** or the **OCI**. The scope of the OCI includes a [Container Image Format Specification](https://github.com/opencontainers/image-spec/blob/master/README.md), which defines the on-disk format for container images as well as the meta-data which defines things like hardware architecture and the operating system (Linux, Windows, etc). An industry wide container image format enables ecosystems of software to flourish - different individual contributors, projects, and vendors are able to build images and tooling, which are interoperable. Users want interoperability between tools for signing, scanning, building, running, moving and managing container images.\n",
    "\n",
    "There are also several competing Container Engines including [Docker](https://www.docker.com/), [CRI-O](http://cri-o.io/), [Railcar](https://github.com/oracle/railcar), [RKT](https://github.com/coreos/rkt), [LXC](https://linuxcontainers.org/lxc/introduction/). These Container Engines take a **Container Image** and turn it into a **Container** (aka running processes). How this happens is governed by the [scope of the OCI](https://www.opencontainers.org/about/oci-scope-table) which includes a [Container Runtime Specification](https://github.com/opencontainers/runtime-spec/blob/master/README.md) and a [Reference Runtime Implementation](https://github.com/opencontainers/runc) called [RunC](https://github.com/opencontainers/runc). This reference implementation is open source, governed by a community development model, and commonly used by many container engines to communicate with the host kernel when creating containers.\n",
    "\n",
    "Tools which target the OCI [Container Image Format Specification](https://github.com/opencontainers/image-spec/blob/master/README.md) and [Container Runtime Specification](https://github.com/opencontainers/runtime-spec/blob/master/README.md) ensure portability between a broad ecosystem of container platforms, container engines, and supporting tools across cloud providers and on premise architectures. \n",
    "\n",
    "Understanding the \n",
    "- nomenclature, \n",
    "- container standards, and \n",
    "- the architecture of the building blocks of containers \n",
    "\n",
    "will ensure that you can communicate with other architects to build scalable & supportable containerized applications and environments to productively run containers for years to come."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97b6a10-430c-4da3-98dd-44328d1d6ea5",
   "metadata": {},
   "source": [
    "![](./data/images/user-space-vs-kernel-space-simple-container.png)\n",
    "\n",
    "[Image source](https://www.redhat.com/en/blog/architecting-containers-part-1-why-understanding-user-space-vs-kernel-space-matters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50220cdf-6951-46d0-9e61-82b57a02f067",
   "metadata": {},
   "source": [
    "![](./data/images/user-space-vs-kernel-space-virtualization-vs-containerization11.png)\n",
    "\n",
    "[Image source](https://www.redhat.com/en/blog/architecting-containers-part-2-why-user-space-matters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9b4f39-6567-4634-a398-658cc8f36f24",
   "metadata": {},
   "source": [
    "# <b>Basic Vocabulary</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65023c7a-6783-46ea-95e8-1566d80d64a4",
   "metadata": {},
   "source": [
    "## Container Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d4ec00-e19a-4bb7-90ea-5efe6b11e098",
   "metadata": {},
   "source": [
    "See also **Repository**.\n",
    "\n",
    "A **container image**, in its simplest definition, is a file which is pulled down from a **Registry Server** and used locally as a mount point when starting Containers. The container community uses “container image” quite a bit, but this nomenclature can be quite confusing. Docker, RKT, and even LXD, operate on the concept of pulling remote files and running them as a Container. Each of these technologies treats container images in different ways. LXD pulls a single container image (single **layer**), while docker and RKT use OCI-based images which can be made up of multiple layers.\n",
    "\n",
    "Technically, it is much more complicated than a single file on a Registry Server. When people use the term \"container image,\" they often mean to imply **Repository** and referring to a bundle of multiple container **Image Layers** as well as **metadata** which provides extra information about the layers.\n",
    "\n",
    "Implicit in the concept of a container image is the concept of a **Container Image Format**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575551c7-33fe-4e58-981e-8a6478370e00",
   "metadata": {},
   "source": [
    "## Container Image Format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4094da4-90b4-4197-941b-e05c19d47180",
   "metadata": {},
   "source": [
    "See **Container Image** and **Background**.\n",
    "\n",
    "Historically, each Container Engine had its container images format. LXD, RKT, and Docker all had their own image formats. Some were made up of a single layer, while others were made up of a bunch of layers in a tree structure. Today, almost all major tools and engines have moved to a format defined by the Open Container Initiative (OCI).This image format defines \n",
    "- the layers and \n",
    "- metadata \n",
    "\n",
    "within a container image. Essentially, \n",
    "\n",
    "> the **OCI image format** defines    \n",
    "    - a **container image** composed of `tar` files for each layer, and   \n",
    "    - a `manifest.json` file with the **metadata**.\n",
    "\n",
    "The Open Container Initiative (OCI), which was originally based on the [Docker V2 image format](https://blog.docker.com/2017/07/demystifying-open-container-initiative-oci-specifications/), has successfully unified a wide ecosystem of container engines, cloud providers and tools providers (security scanning, signing, building and moving). This will help protect users as the invest in knowledge, and tooling in their environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6891e8a-f918-4642-a5d2-775632fe68df",
   "metadata": {},
   "source": [
    "## Container Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296de9d0-ea81-4327-8af8-b1fb9a0941b8",
   "metadata": {},
   "source": [
    "See also **Container Runtime**.\n",
    "\n",
    "A **container engine** is a piece of software that accepts user requests, including command line options, pulls images, and from the end user's perspective runs the container. \n",
    "\n",
    "There are many container engines, including docker, RKT, CRI-O, and LXD. Also, many cloud providers, Platforms as a Service (PaaS), and Container Platforms have their own built-in container engines which consume Docker or OCI compliant Container Images. Having an industry standard Container Image Format allows interoperability between all of these different platforms.\n",
    "\n",
    "Going one layer deeper, most container engines don't actually run the containers, they rely on an OCI compliant runtime like **runc**. Typically, the container engine is responsible for:\n",
    "\n",
    "- Handling user input\n",
    "- Handling input over an API often from a Container Orchestrator\n",
    "- Pulling the Container Images from the Registry Server\n",
    "- Expanding decompressing and expanding the container image on disk using a Graph Driver (block, or file depending on driver)\n",
    "- Preparing a container mount point, typically on copy-on-write storage (again block or file depending on driver)\n",
    "- Preparing the metadata which will be passed to the container **Container Runtime** to start the Container correctly\n",
    "    - Using some defaults from the container image (ex.ArchX86)\n",
    "    - Using user input to override defaults in the container image (ex. CMD, ENTRYPOINT)\n",
    "    - Using defaults specified by the container image (ex. SECCOM rules)\n",
    "- Calling the Container Runtime\n",
    "\n",
    "For an even deeper understanding, please see [Understanding the Container Standards](https://docs.google.com/presentation/d/1OpsvPvA82HJjHN3Vm2oVrqca1FCfn0PAfxGZ2w_ZZgc/edit#slide=id.g2441f8cc8d_0_80). See also **Container Runtime**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089b3420-a309-4aa1-932b-b3b95e9ac71c",
   "metadata": {},
   "source": [
    "## Container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604d3c06-fdc3-4a3f-b286-4e3fec11ef8d",
   "metadata": {},
   "source": [
    "Containers have existed within operating systems for quite a long time. \n",
    "\n",
    "> A **container** is the runtime instantiation of a Container Image. \n",
    "\n",
    "A container is a standard Linux process typically created through a `clone()` system call instead of `fork()` or `exec()` (Podman behaves differently, see [Podman in Action](./01_podman/02_podman_in_action.ipynb#1.3-Fork/exec-model)). Also, containers are often isolated further through the use of [cgroups](https://en.wikipedia.org/wiki/Cgroups), [SELinux](https://en.wikipedia.org/wiki/Security-Enhanced_Linux) or [AppArmor](https://en.wikipedia.org/wiki/AppArmor).\n",
    "\n",
    "[Additional info](http://rhelblog.redhat.com/2015/09/17/architecting-containers-part-2-why-the-user-space-matters-2/):\n",
    "\n",
    "Notice in the drawing below that when a container is created, the `clone()` system call is used to create a new process. Clone is similar to `fork`, but allows the new process to be set up within a **kernel namespace**. Kernel namespaces allow the new process to have its own \n",
    "- hostname, \n",
    "- IP address, \n",
    "- filesystem mount points, \n",
    "- process id, \n",
    "\n",
    "and more. A new namespace can be created for each new container allowing them to each look and feel similar to a virtual machine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5b8fe6-a3a5-4bb8-ac89-2e8a5b0dd40f",
   "metadata": {},
   "source": [
    "![](./data/images/q9ul7asyd20ccgzv9sfdbfb7daaifmmzj73umlq-kzw-foekes2lmwubyrqxihp-_gkvdtvglyqqk_nljtw5c2pv48a-ta2li0x2i5chnzayecxhvtrfylckq1vlnft1sjxg0py.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0897204f-6757-43dd-889a-6cda5ea092f6",
   "metadata": {},
   "source": [
    "Once the container is instantiated, the process or processes execute within a pristine user space created from mounting the container image. The processes inside the container make system calls as they would normally. The kernel is responsible for limiting what the processes in the container can do. Notice from the drawing below, that \n",
    "- the first command executes the `open()` system call directly from the host’s user space, \n",
    "- the second command executes the `open()` system call through the mount namespace (containerized), and \n",
    "- the third executes the `getpid()` through the PID namespace (containerized)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6fda73-66c9-496b-ab72-ea2ff86b0aa3",
   "metadata": {},
   "source": [
    "![](./data/images/user-space-vs-kernel-space-inside-container-system-calls.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c054dafc-96ef-475c-9204-afcdfdf5b6d6",
   "metadata": {},
   "source": [
    "When the container is stopped, the kernel name space count is decremented and typically removed. Once terminated, the user has the option of \n",
    "- discarding the work done, or \n",
    "- saving the container as a new image.\n",
    "\n",
    "As long as infrastructure parity (CPU, memory, network, kernel compatibility) is achieved between environments, the same container image can be started and run on a developer's laptop, servers in the datacenter, or on virtual machines in the cloud. Containers have the advantage of providing developers, architects, quality engineering, release engineers, and systems administrators with a currency for collaboration that has the entire user space packaged and shipped in a convenient and easy to use format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61427791-3e60-4a06-b1d2-f0aedfdd8cca",
   "metadata": {},
   "source": [
    "## Container Host"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5d1a61-a5d5-4dda-97b0-bdd570f9f8a2",
   "metadata": {},
   "source": [
    "The **container host** is the system that runs the containerized processes, often simply called **containers**. This could be, for example, RHEL Atomic Host running in a VM, as an instance in the public cloud, or on bare metal in your data center. Once a **container image** (aka repository) is pulled from a Registry Server to the local container host, it is said to be in the local cache.\n",
    "\n",
    "Determining which repositories are synchronized to the local cache can be done with the following command:\n",
    "\n",
    "```\n",
    "[root@rhel7 ~]# docker images -a\n",
    "\n",
    "REPOSITORY                             TAG                     IMAGE ID                CREATED                 VIRTUAL SIZE\n",
    "registry.access.redhat.com/rhel7   latest                  6883d5422f4e            3 weeks ago             201.7 MB\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb81bb5-a8ce-44b4-9af8-e20e6685cf61",
   "metadata": {},
   "source": [
    "## Registry Server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69ad695-3d52-4155-b0a1-4067ccf958de",
   "metadata": {},
   "source": [
    "A **registry server** is essentially a fancy file server that is used to store docker repositories. Typically, the registry server is specified as a normal DNS name and optionally a port number to connect to. Much of the value in the docker ecosystem comes from the ability to push and pull repositories from registry servers.\n",
    "\n",
    "When a docker daemon does not have a locally cached copy of a repository, it will automatically pull it from a registry server. Most Linux distributions have the docker daemon configured to pull from `docker.io` but it is configurable on some Linux distributions. For example, Red Hat Enterprise Linux is configured to pull repositories from `registry.access.redhat.com` first, then it will try `docker.io` (Docker Hub).\n",
    "\n",
    "It is important to stress that there is _implicit trust_ in the registry server. You must determine how much you trust the content provided by the registry and you may want to allow or block certain registries. In addition to security, there are other concerns such as users having access to licensed software and compliance issues. The simplicity with which docker allows users to pull software makes it critical that you trust upstream content.\n",
    "\n",
    "In Red Hat Enterprise Linux, the default docker registry is configurable. Specific registry servers can be added or blocked in RHEL7 and RHEL7 Atomic by modifying the configuration file:\n",
    "\n",
    "```\n",
    "vi /etc/sysconfig/docker\n",
    "```\n",
    "In RHEL7 and RHEL 7 Atomic, Red Hat’s registry server is configured out of the box:\n",
    "\n",
    "```\n",
    "ADD_REGISTRY='--add-registry registry.access.redhat.com'\n",
    "```\n",
    "\n",
    "As a matter of security, it may be useful to block public docker repositories such as DockerHub:\n",
    "\n",
    "```\n",
    "# BLOCK_REGISTRY='--block-registry'\n",
    "```\n",
    "\n",
    "Red Hat also offers an integrated Registry Server with [OpenShift Container Platform](https://docs.openshift.com/container-platform/3.7/install_config/registry/index.html), a standalone enterprise Registry Server with [Quay Enterprise](https://quay.io/plans/?tab=enterprise), as well as cloud based, public and private repositories on [Quay.io](https://quay.io/plans/).\n",
    "\n",
    "ChatGPT:\n",
    "\n",
    "The concern about security risks associated with public Docker repositories such as Docker Hub is indeed real. These repositories can potentially be sources of unverified or malicious container images, posing security threats to organizations that use them.\n",
    "\n",
    "To address these concerns, organizations may consider the following alternatives for Docker Hub:\n",
    "\n",
    "- **Private Repositories:** Maintain private repositories within the organization's infrastructure or on trusted third-party platforms. This allows organizations to control and verify the container images they use, reducing the potential security risks associated with public repositories.\n",
    "\n",
    "- **Trusted Registries:** Use trusted container registries that enforce strict security measures and provide mechanisms for image verification and validation. Examples of such registries include Red Hat Quay, Harbor, and AWS Elastic Container Registry (ECR).\n",
    "\n",
    "- **Image Scanning and Security Tools:** Implement image scanning and security tools that can automatically analyze container images for vulnerabilities and compliance issues before they are deployed. Tools such as Clair and Anchore can help organizations ensure the security of container images.\n",
    "\n",
    "- **Self-Hosting:** Host container images on-premises or in a cloud environment under the organization's control, thus minimizing reliance on external repositories and mitigating potential security risks.\n",
    "\n",
    "In 2024, the landscape of container image repositories and security practices may have evolved further, with new solutions and best practices emerging to address the ongoing concerns related to the security of containerized applications. Organizations should stay updated on the latest security trends and adapt their container image management strategies accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1360d3f5-46fb-4321-bad8-26900ba09a06",
   "metadata": {},
   "source": [
    "## Container Orchestration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f31db6-3d65-405f-b951-6e44fb81018f",
   "metadata": {},
   "source": [
    "Often teams start with installing a Container Host, then pulling some Container Images. Then they move on to building some new Container Images and pushing them to a Registry Server to share with others on their team. After a while they want to wire a few containers together and deploy them as a unit. Finally, at some point, they want to push that unit into a pipeline (Dev/QA/Prod) leading towards production. This is the path towards the realization that orchestration is needed.\n",
    "\n",
    "A **container orchestrator** really does two things:\n",
    "\n",
    "- Dynamically schedules container workloads within a cluster of computers. This is often referred to as **distributed computing**.\n",
    "- Provides a standardized application definition file (kube yaml, docker compose, etc)\n",
    "\n",
    "The above two features provide many capabilities:\n",
    "\n",
    "- Allows containers within an application to be scheduled completely separately. This is useful if:\n",
    "    - Allows the utilization of large clusters of Container Hosts\n",
    "    - Individual containers fail (process hang, out of memory (OOM))\n",
    "    - Container Hosts fail (disk, network, reboot)\n",
    "    - Container Engines fail (corruption, restart)\n",
    "    - Individual containers need to be scaled up, or scaled down\n",
    "- It's easy to deploy new instances of the same application into new environments. In a cloud native world, or traditional world, there are many reasons why you might want to do this including:\n",
    "    - On developers laptop with a container orchestrator running\n",
    "    - On a shared development environment in a private namespace\n",
    "    - On a shared development environment in an internally public namespace for visibility and testing\n",
    "    - On a quality assurance (QA) environment internally\n",
    "    - On a load test environment dynamically provisioned and deprovisioned in the cloud\n",
    "    - On a gold environment to test compatibility with production\n",
    "    - On a production environment\n",
    "    - On a disaster recovery environment\n",
    "    - On a new production environment which has upgraded Container Hosts, Container Engines, or container orchestrators\n",
    "    - On a new production environment with the same versions of Container Host, Container Engine, and container orchestrator in a new geolocation (APAC, EMEA, etc)\n",
    "\n",
    "There are many container schedulers being developed in the community and by vendors. Historically, Swarm, Mesos, and Kubernetes were the big three, but recently even Docker and Mesosphere have announced support for Kubernetes - as has almost every major cloud service provider. Kubernetes has become the defacto standard in container orchestration, similar to Linux before it. If you are looking at container orchestration, Red Hat recommends our enterprise distribution called OpenShift."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5415158-7214-49e0-b758-f5edd9dd049a",
   "metadata": {},
   "source": [
    "# <b>Advanced Vocabulary</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7a3854-747e-4aec-aa04-3a68bcae2af9",
   "metadata": {},
   "source": [
    "## Container Runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b73dff1-ffd4-4a11-8dc0-143cb5d529f9",
   "metadata": {},
   "source": [
    "A **container runtime**, a lower level component, typically used in a Container Engine but can also be used by hand for testing. The Open Containers Initiative (OCI) Runtime Standard reference implementation  is `runc`. This is the most widely used container runtime, but there are others OCI compliant runtimes, such as `crun`, `railcar`, and `katacontainers`. Docker, CRI-O, and many other Container Engines rely on `runc`.\n",
    "\n",
    "The container runtime is responsible for:\n",
    "\n",
    "- Consuming the container mount point provided by the Container Engine (can also be a plain directory for testing)\n",
    "- Consuming the container metadata provided by the Container Engine (can a also be a manually crafted `config.json` for testing)\n",
    "- Communicating with the kernel to start containerized processes (`clone` system call)\n",
    "- Setting up cgroups\n",
    "- Setting up SELinux Policy\n",
    "- Setting up App Armor rules\n",
    "\n",
    "To provide a bit of history, when the Docker engine was first created it relied on LXC as the container runtime. Later, the Docker team developed their own library called `libcontainer` to start containers. This library was written in Golang, and compiled into the original Docker engines. Finally, when the OCI was created, Docker donated the `libcontainer` code and turned it into a standalone utility called `runc`. Now, `runc` is the reference implementation and used by other Container Engines such as CRI-O. At the lowest level, this provides the ability to start a container consistently, no matter the container engine. Runc is a very terse utility and expects a mount point (directory) and meta-data (`config.json`) to be provided to it. See [this tutorial](https://github.com/opencontainers/runc/blob/master/README.md) for more on `runc`.\n",
    "\n",
    "For an even deeper understanding, please see [Understanding the Container Standards](https://docs.google.com/presentation/d/1OpsvPvA82HJjHN3Vm2oVrqca1FCfn0PAfxGZ2w_ZZgc/edit#slide=id.g2441f8cc8d_0_80). See also Container Runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87152789-c59b-4658-9333-7f4af5656451",
   "metadata": {},
   "source": [
    "## Repository"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eff260d-76b1-43e5-b636-629747178a3a",
   "metadata": {},
   "source": [
    "When using the `docker` command, a **repository** is what is specified on the command line, not an image. In the following command, “rhel7” is the repository.\n",
    "\n",
    "```\n",
    "docker pull rhel7\n",
    "```\n",
    "\n",
    "This is actually expanded automatically to:\n",
    "\n",
    "```\n",
    "docker pull registry.access.redhat.com/rhel7:latest\n",
    "```\n",
    "\n",
    "This can be confusing, and many people refer to this as an image or a container image. In fact, the docker images sub-command is what is used to list the locally available repositories. Conceptually, these repositories can be thought of as container images, but it’s important to realize that these repositories are actually made up of layers and include metadata about in a file referred to as the manifest (`manifest.json`):\n",
    "\n",
    "```\n",
    "docker images\n",
    "\n",
    "REPOSITORY                                  TAG                     IMAGE ID                CREATED                 VIRTUAL SIZE\n",
    " registry.access.redhat.com/rhel7            latest                  6883d5422f4e            4 weeks ago             201.7 MB\n",
    " registry.access.redhat.com/rhel             latest                  6883d5422f4e            4 weeks ago             201.7 MB\n",
    " registry.access.redhat.com/rhel6            latest                  05c3d56ba777            4 weeks ago             166.1 MB\n",
    " registry.access.redhat.com/rhel6/rhel       latest                  05c3d56ba777            4 weeks ago             166.1 MB\n",
    " ...\n",
    "```\n",
    "\n",
    "When we specify the repository on the command line, the Container Engine is doing some extra work for you. In this case, the docker daemon (not the client tool) is configured with a list of servers to search. In our example above, the daemon will search for the “rhel7” repository on each of the configured servers.\n",
    "\n",
    "In the above command, only the repository name was specified, but it’s also possible to specify a full URL with the docker client. To highlight this, let’s start with dissecting a full URL.\n",
    "\n",
    "Another way you will often see this specified is:\n",
    "\n",
    "```\n",
    "REGISTRY/NAMESPACE/REPOSITORY[:TAG]\n",
    "```\n",
    "\n",
    "The full URL is made up of a standard server name, a namespace, and optionally a tag. There are actually many permutations of how to specify a URL and as you explore the docker ecosystem, you will find that many pieces are optional. The following commands are all valid and all pull some permutation of the same repository:\n",
    "\n",
    "```\n",
    " docker pull registry.access.redhat.com/rhel7/rhel:latest\n",
    " docker pull registry.access.redhat.com/rhel7/rhel\n",
    " docker pull registry.access.redhat.com/rhel7\n",
    " docker pull rhel7/rhel:latest\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad370eb9-b479-431b-b808-f8923d66a314",
   "metadata": {},
   "source": [
    "## Image Layer (aka blob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747a57da-c51e-428b-ba51-47b247bf89d9",
   "metadata": {},
   "source": [
    "**Repositories** are often referred to as **images** or **container images**, but actually they are made up of one or more layers. **Image layers** in a repository are connected together in a _parent-child relationship_. Each image layer represents changes between itself and the parent layer.\n",
    "\n",
    "Below, we are going to inspect the layers of a repository on the local container host. Since Docker 1.7, there is no native tooling to inspect image layers in a local repository (there are tools for online registries). With the help of a tool called `Dockviz`, you can quickly inspect all of the layers (also see **Skopeo**). \n",
    "\n",
    "Notice that \n",
    "\n",
    "> each layer has  \n",
    "    - **tag** and  \n",
    "    - a Universally Unique Identifier (**UUID**). \n",
    "\n",
    "The following command will return shortened versions of the UUID that are typically unique enough to work with on a single machine. If you need the full UUID, use the `--no-trunc` option.\n",
    "\n",
    "```sh\n",
    "docker run --rm --privileged -v /var/run/docker.sock:/var/run/docker.sock nate/dockviz images -t\n",
    "```\n",
    "```\n",
    "├─2332d8973c93 Virtual Size: 187.7 MB\n",
    " │ └─ea358092da77 Virtual Size: 187.9 MB\n",
    " │   └─a467a7c6794f Virtual Size: 187.9 MB\n",
    " │         └─ca4d7b1b9a51 Virtual Size: 187.9 MB\n",
    " │           └─4084976dd96d Virtual Size: 384.2 MB\n",
    " │             └─943128b20e28 Virtual Size: 386.7 MB\n",
    " │               └─db20cc018f56 Virtual Size: 386.7 MB\n",
    " │                 └─45b3c59b9130 Virtual Size: 398.2 MB\n",
    " │                   └─91275de1a5d7 Virtual Size: 422.8 MB\n",
    " │                     └─e7a97058d51f Virtual Size: 422.8 MB\n",
    " │                       └─d5c963edfcb2 Virtual Size: 422.8 MB\n",
    " │                         └─5cfc0ce98e02 Virtual Size: 422.8 MB\n",
    " │                           └─7728f71a4bcd Virtual Size: 422.8 MB\n",
    " │                             └─0542f67da01b Virtual Size: 422.8 MB Tags: docker.io/registry:latest\n",
    "```\n",
    "\n",
    "Notice, that the “`docker.io/registry`” repository is actually made up of many images layers. More importantly, notice that a user could potentially “run” a container based off of any one of these layers. The following command is perfectly valid, though not guaranteed to have been tested or actually even work correctly. Typically, an image builder will tag (create a name for) specific layers that you should use:\n",
    "\n",
    "```sh\n",
    "docker run -it 45b3c59b9130 bash\n",
    "```\n",
    "\n",
    "Repositories are constructed this way because whenever an image builder creates a new image, the differences are saved as a layer. \n",
    "\n",
    "There are two main ways that new layers are created in a repository. \n",
    "- First, if building an image manually, each “commit” creates a new layer. \n",
    "- If the image builder is building an image with a Dockerfile, each directive in the file creates a new layer. It is useful to have visibility into what has changed in a container repository between each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d07a81-3e6f-4320-b7c8-32f096485975",
   "metadata": {},
   "source": [
    "## Tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b765f0b6-1bed-4047-9d9e-9cc935b77f84",
   "metadata": {},
   "source": [
    "Even though a user can designate that a container mount and start from any layer in a repository, they shouldn’t necessarily do that. When an image builder creates a new repository, they will typically label the best image layers to use. These are called tags and are a tool for container image builders to communicate to container image consumers which layers are best to consume. Typically, **tags** are used to designate versions of software within in the repository. This is by convention only - in reality, the OCI nor any other standard mandates what tags can be used for, and they can be abused for anything that a user wants. Be careful doing this because it can create a lot of confusion in development, operations and architecture teams, so document it well if you use it for anything other than software version.\n",
    "\n",
    "There is one special tag - `latest` - which typically points to the layer containing the latest version of software in the repository. This special tag still just points to an image layer, like any other tag, so it can be abused.\n",
    "\n",
    "To remotely view the tags available in a repository, run the following command (the `jq` utility makes the output a lot more readable):\n",
    "\n",
    "```\n",
    "curl -s registry.access.redhat.com/v1/repositories/rhel7/tags | jq\n",
    " {\n",
    " \"7.0-21\": \"e1f5733f050b2488a17b7630cb038bfbea8b7bdfa9bdfb99e63a33117e28d02f\",\n",
    " \"7.0-23\": \"bef54b8f8a2fdd221734f1da404d4c0a7d07ee9169b1443a338ab54236c8c91a\",\n",
    " \"7.0-27\": \"8e6704f39a3d4a0c82ec7262ad683a9d1d9a281e3c1ebbb64c045b9af39b3940\",\n",
    " \"7.1-11\": \"d0a516b529ab1adda28429cae5985cab9db93bfd8d301b3a94d22299af72914b\",\n",
    " \"7.1-12\": \"275be1d3d0709a06ff1ae38d0d5402bc8f0eeac44812e5ec1df4a9e99214eb9a\",\n",
    " \"7.1-16\": \"82ad5fa11820c2889c60f7f748d67aab04400700c581843db0d1e68735327443\",\n",
    " \"7.1-24\": \"c4f590bbcbe329a77c00fea33a3a960063072041489012061ec3a134baba50d6\",\n",
    " \"7.1-4\": \"10acc31def5d6f249b548e01e8ffbaccfd61af0240c17315a7ad393d022c5ca2\",\n",
    " \"7.1-6\": \"65de4a13fc7cf28b4376e65efa31c5c3805e18da4eb01ad0c8b8801f4a10bc16\",\n",
    " \"7.1-9\": \"e3c92c6cff3543d19d0c9a24c72cd3840f8ba3ee00357f997b786e8939efef2f\",\n",
    " \"7.2\": \"6c3a84d798dc449313787502060b6d5b4694d7527d64a7c99ba199e3b2df834e\",\n",
    " \"7.2-2\": \"58958c7fafb7e1a71650bc7bdbb9f5fd634f3545b00ec7d390b2075db511327d\",\n",
    " \"7.2-35\": \"6883d5422f4ec2810e1312c0e3e5a902142e2a8185cd3a1124b459a7c38dc55b\",\n",
    " \"7.2-38\": \"6c3a84d798dc449313787502060b6d5b4694d7527d64a7c99ba199e3b2df834e\",\n",
    " \"latest\": \"6c3a84d798dc449313787502060b6d5b4694d7527d64a7c99ba199e3b2df834e\"\n",
    " }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8328466a-9e82-4d92-a3bb-9d5758707186",
   "metadata": {},
   "source": [
    "## Namespace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f2582e-06d8-4ce1-b603-2d85e3a13abf",
   "metadata": {},
   "source": [
    "A **namespace** is a tool for separating groups of repositories. On the public DockerHub, the namespace is typically the username of the person sharing the image, but can also be a group name, or a logical name.\n",
    "\n",
    "Red Hat uses the namespace to separate groups of repositories based on products listed on the Red Hat Federated Registry server. Here are some example results returned by `registry.access.redhat.com`. Notice, the last result is actually listed on another registry server. This is because Red Hat works to also list repositories on our partner’s registry servers:\n",
    "\n",
    "```\n",
    "registry.access.redhat.com/rhel7/rhel\n",
    "registry.access.redhat.com/openshift3/mongodb-24-rhel7\n",
    "registry.access.redhat.com/rhscl/mongodb-26-rhel7\n",
    "registry.access.redhat.com/rhscl_beta/mongodb-26-rhel7\n",
    "registry-mariadbcorp.rhcloud.com/rhel7/mariadb-enterprise-server:10.0\n",
    "```\n",
    "\n",
    "Notice, that sometimes the full URL does not need to be specified. In this case, there is a default repository for a given namespace. If a user only specifies the `fedora` namespace, the latest tag from the default repository will be pulled to the local server. So, running the following commands is essentially the same, each one more specific:\n",
    "\n",
    "```\n",
    "docker pull fedora\n",
    "docker pull docker.io/fedora\n",
    "docker pull docker.io/library/fedora:latest\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6294d4b8-db9c-4bd9-b40b-5b99b53b1973",
   "metadata": {},
   "source": [
    "## Kernel Namespace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d86dfb8-6eca-4df3-bca3-13034fc7a3fd",
   "metadata": {},
   "source": [
    "A **kernel namespace** is completely different than the namespace we are referring to when discussing Repositories and Registry Servers. When discussing containers, Kernel namespaces are perhaps the most important data structure, because they enable containers as we know them today. \n",
    "\n",
    "> Kernel namespaces enable each container to have it's own mount points, network interfaces, user identifiers, process identifiers, etc.\n",
    "\n",
    "When you type a command in a Bash terminal and hit enter, Bash makes a request to the kernel to create a normal Linux process using a version of the `exec()` system call. \n",
    "\n",
    "A container is special because when you send a request to a container engine like docker, the docker daemon makes a request to the kernel to create a containerized process using a different system call called `clone()`. This `clone()` system call is special because it can create a process with its own virtual mount points, process ids, user ids, network interfaces, hostname, etc.\n",
    "\n",
    "While, technically, there is no single data structure in Linux that represents a container, kernel namespaces and the `clone()` system call are as close as it comes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1df3f8-5d04-459c-9ec8-d3ea439f86e1",
   "metadata": {},
   "source": [
    "![](./data/images/q9ul7asyd20ccgzv9sfdbfb7daaifmmzj73umlq-kzw-foekes2lmwubyrqxihp-_gkvdtvglyqqk_nljtw5c2pv48a-ta2li0x2i5chnzayecxhvtrfylckq1vlnft1sjxg0py.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9ace81-b354-4b37-99e7-7cff8126460f",
   "metadata": {},
   "source": [
    "## Graph Driver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82661824-609f-47c3-83cc-f3412d0e29b0",
   "metadata": {},
   "source": [
    "When the end user specifies the Tag of a container image to run - by default this is the latest Tag - the graph driver unpacks all of the dependent Image Layers necessary to construct the data in the selected Tag. \n",
    "\n",
    "> The **graph driver** is the piece of software that maps the necessary image layers in the Repository to a piece of local storage. \n",
    "\n",
    "The container image layers can be mapped to a directory using a driver like Overlay2 or in block storage using a driver like Device Mapper. Drivers include: \n",
    "- aufs, \n",
    "- devicemapper, \n",
    "- btrfs, \n",
    "- zfs, and \n",
    "- overlayfs.\n",
    "\n",
    "When the container is started, the image layers are mounted read-only with a kernel namespace. The Image Layers from the Repository are always mounted read only but by default, a separate **copy-on-write layer** is also set up. This allows the containerized process to write data within the container. When data is written, it is stored in the copy-on-write layer, on the underlying host. This copy-on-write layer can be disabled by running the container with an option such as `--readonly`.\n",
    "\n",
    "The docker daemon has it's own set of Graph Drivers and there are other open source libraries which provide Graph Drivers such as containers/images which is used in tools like CRI-O, Skopeo and other container engines.\n",
    "\n",
    "Determining which graph driver you are using can be done with the docker info command:\n",
    "\n",
    "```\n",
    "[root@rhel7 ~]# docker info\n",
    "\n",
    "...\n",
    " Storage Driver: devicemapper\n",
    " Pool Name: docker-253:1-884266-pool\n",
    " Pool Blocksize: 65.54 kB\n",
    " Backing Filesystem: xfs\n",
    " Data file: /dev/loop0\n",
    " Metadata file: /dev/loop1\n",
    " Data Space Used: 3.037 GB\n",
    " Data Space Total: 107.4 GB\n",
    " Data Space Available: 2.56 GB\n",
    " Metadata Space Used: 2.707 MB\n",
    " Metadata Space Total: 2.147 GB\n",
    " Metadata Space Available: 2.145 GB\n",
    " Udev Sync Supported: true\n",
    " Deferred Removal Enabled: false\n",
    " Data loop file: /var/lib/docker/devicemapper/devicemapper/data\n",
    " Metadata loop file: /var/lib/docker/devicemapper/devicemapper/metadata\n",
    " Library Version: 1.02.107-RHEL7 (2015-10-14)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa45bf7-8e8b-45b4-8f4c-f6ec1e0f66a6",
   "metadata": {},
   "source": [
    "# <b>Container Use Cases</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bddecb-c395-4bb4-a49f-ceeea26a917e",
   "metadata": {},
   "source": [
    "There are many types of Container design patterns forming. Since containers are the run time version of a container image, the way it is built is tightly coupled to how it is run.\n",
    "\n",
    "Some Container Images are designed to be run without privilege, while others are more specialized and require root-like privileges. There are many dimensions in which patterns can be evaluated and often users will see multiple patterns or use cases tackled together in one container `image/container`.\n",
    "\n",
    "This section will delve into some of the common use cases that users are tackling with containers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a565ed6f-26f0-41c5-9e0b-c8340b7fbcc1",
   "metadata": {},
   "source": [
    "## Application Containers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ef1fbc-5516-453a-8426-3294b5509458",
   "metadata": {},
   "source": [
    "**Application containers** are the most popular form of container. These are what developers and application owner's care about. Application containers contain the code that developers work on. They also include things like MySQL, Apache, MongoDB, and or Node.js.\n",
    "\n",
    "There is a great ecosystem of application containers forming. Projects like Software Collections are providing secure and supportable applications container images for use with Red Hat Enterprise Linux. At the same time, Red Hat community members are driving some great cutting edge applications containers.\n",
    "\n",
    "Red Hat believes that Application Containers should not typically require special privileges to run their workloads. That said, production container environments typically require much more than just non-privileged application containers to provide other supporting services."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b2169f-c202-40e3-9e0c-9d85da4d0ede",
   "metadata": {},
   "source": [
    "## Operating System Containers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db06d64f-40f5-4301-8a06-ae1b4e573e3e",
   "metadata": {},
   "source": [
    "See also **System Containers**\n",
    "\n",
    "**Operating System Containers** are containers which are treated more like a full virtual operating system. Operating System Containers still share a host kernel, but run a full init system which allows them to easily run multiple processes. LXC and LXD are examples of Operating System Containers because they are treated much like a full virtual machine.\n",
    "\n",
    "It is also possible to approximate an Operating System Container with docker/OCI based containers, but requires running `systemd` inside the container. This allows an end user to install software like they normally would and treat the container much more like a full operating system.\n",
    "\n",
    "```\n",
    "yum install mysql\n",
    "systemctl enable mysql\n",
    "```\n",
    "\n",
    "This makes it easier to migrate existing applications. Red Hat is working hard to make Operating System Containers easier by enabling `systemd` to run inside a container and by enabling management with machined. While many customers aren't (yet) ready to adopt micro-services, they can still gain benefits from adopting image based containers as a software delivery model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1e6a10-04dd-47f8-be79-2d51a81cec5d",
   "metadata": {},
   "source": [
    "## Pet Containers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761d1e5a-478d-4f0c-996a-de9ad76dc3ab",
   "metadata": {},
   "source": [
    "While Red Hat certainly recommends, supports and evangelizes the use cloud native patterns for new application development, in reality not all existing applications will be rewritten to take advantage of new patterns. Many existing applications are one of a kind, and one of kind applications are often referred to as Pets. Containers built specifically to handle these pet applications are sometimes referred to as **Pet Containers**.\n",
    "\n",
    "Pet containers provide users with the portability and convenience of a standardized container infrastructure relying on registry servers, container images, and standard container hosts for infrastructure, but provide the flexibility of a traditional environment within the container. The idea is to make things easier to containerize existing applications, such as using `systemd` in a container. The goal is to reuse existing automation, installers, and tools to easily create a container image that just runs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b70495-b571-427c-a7be-3e6b205f62f6",
   "metadata": {},
   "source": [
    "## Super Privileged Containers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab47763e-ce82-4746-b444-f87c0bf5e2b8",
   "metadata": {},
   "source": [
    "When building container infrastructure on dedicated container hosts such as Red Hat Enterprise Linux Atomic Host, systems administrators still need to perform administrative tasks. Whether used with distributed systems, such as Kubernetes or OpenShift or standalone container hosts, **Super Privileged Containers (SPC)** are a powerful tool. SPCs can even do things like load specialized kernel modules, such as with `systemtap`.\n",
    "\n",
    "In an infrastructure that is built to run containers, administrators will most likely need SPCs to do things like monitoring, backups, etc. It's important to realize that there is typically a tighter coupling between SPCs and the host kernel, so administrators need to choose a rock solid container host and standardize on it, especially in a large clustered/distributed environment where things are more difficult to troubleshoot. They then need to select a user space in the SPC that is compatible with the host kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17355ff3-3558-425d-b960-f82c4d9aceb5",
   "metadata": {},
   "source": [
    "## Tools & Operating System Software"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7c0407-26f9-47b9-b600-6734b9e23298",
   "metadata": {},
   "source": [
    "Linux distributions have always provided users with system software such as `Rsyslogd`, `SSSD`, `sadc`, etc. Historically, these pieces of system software were installed through RPM or DEB packages. But with the advent of containers as a packaging format, it has become both convenient and easy to install system software through containers images. Red Hat provides some pre-packaged containers for things like the Red Hat Virtualization tools, rsyslog, sssd, and sadc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf69664a-75b6-412f-9431-aa763b7b4965",
   "metadata": {},
   "source": [
    "# <b>Architecture of Containers</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88906d10-66d0-4bc1-ae9e-61a0fd409a22",
   "metadata": {},
   "source": [
    "New design patterns are forming as more and more people deliver software with containers.  Red Hat engineering is leveraging and driving many of these patterns in the community. The goal of this section is to help highlight and define some of these patterns.\n",
    "\n",
    "The way a container is saved on disk (i.e. its image format) can have a dramatic affect on how it is run. For example, a container which is designed to run `sssd` needs to have special privileges whenever it's run, or it can't do its job. The following is a short list of patterns that are forming in the container community."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d04f56-4a66-41cb-af95-b40d11b97e74",
   "metadata": {},
   "source": [
    "## Application Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80b5cbd-a2d4-4980-aef1-7994197226f7",
   "metadata": {},
   "source": [
    "These images are what end users consume. Use cases range from databases and web servers, to applications and services buses. These can be built in house or delivered to a customer from an ISV. Often end users will investigate and care about what bits were used to create a standalone image. Standalone images are the easiest kind of image to consume, but the hardest to design, build, and patch.\n",
    "\n",
    "ChatGPT:\n",
    "\n",
    "**ISV** stands for **Independent Software Vendor**. An ISV is a company that develops, markets, and sells software products that run on one or more computer hardware or software platforms. In the context you provided, it means that the images mentioned can be provided to the customer by Independent Software Vendors, which implies that the software and services depicted in the images can either be created internally by the company or acquired from external software vendors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e106d80-8fae-48ee-8bf7-b8ac27df3ab7",
   "metadata": {},
   "source": [
    "## Base Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70a74cd-03b9-4161-9206-178544cd7c55",
   "metadata": {},
   "source": [
    "A base image is one of the simplest types of images, but you will find a lot of definitions. Sometimes users will refer to corporate standard build, or even an application image as the “base image.”  Technically this is not a base image. These are **Intermediate images**.\n",
    "\n",
    "Simply put, \n",
    "\n",
    "> a **base image** is an image that has no parent layer. \n",
    "\n",
    "Typically, a base image contains a fresh copy of an operating system. Base images normally include the tools (`yum`, `rpm`, `apt-get`, `dnf`, `microdnf`) necessary to install packages / make updates to the image over time. While base images can be “hand crafted”, in practice they are typically produced and published by open source projects (like Debian, Fedora or CentOS) and vendors (like Red Hat). The provenance of base images is critical for security. In short, the sole purpose of a base image is to provide a starting place for creating your derivative images. When using a dockerfile, the choice of which base image you are using is explicit:\n",
    "\n",
    "```\n",
    "FROM registry.access.redhat.com/rhel7-atomic\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edff0236-8c13-43cc-b06e-452553b04a6c",
   "metadata": {},
   "source": [
    "## Builder Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0ed405-4dee-45f5-b9a2-3542859ad60c",
   "metadata": {},
   "source": [
    "These are a specialized form of container image which produce application container images as offspring. They include everything but a developer's source code. \n",
    "\n",
    "Builder images include \n",
    "- operating system libraries, \n",
    "- language runtimes, \n",
    "- middleware, and \n",
    "- the source-to-image tooling.\n",
    "\n",
    "When a builder image is run, it injects the developers source code and produces a ready-to-run offspring application container image. This newly created application container image can then be run in development or production.\n",
    "\n",
    "For example, if a developer has PHP code and they want to run it in a container, they can use a PHP builder image to produce a ready to run application container image. The developer passes the GitHub URL where the code is stored and the builder image does the rest of the work for them. The output of a Builder container is an Application container image which includes Red Hat Enterprise Linux, PHP from Software Collections, and the developer’s code, all together, ready to run.\n",
    "\n",
    "Builder images provide a powerful way to go from code to container quickly and easily, building off of trusted components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5927f27f-0965-4ffc-a7dc-b0ae9f37bcab",
   "metadata": {},
   "source": [
    "## Containerized  Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53582665-9212-4cd2-8b55-180f6eb74a6e",
   "metadata": {},
   "source": [
    "A container is meant to be deployed as part of a larger software system, not on its own. Two major trends are driving this.\n",
    "\n",
    "First, microservices are driving the use of best of breed components - this is also driving the use of more components combined together to build a single application. Containerized components are meeting the need to deploy an expanding quantity of complex software more quickly and easily. Each of these components can have different revisions, and container images help enable this. Application definitions such as Kubernetes/OpenShift deployments yaml/json, open service broker, OpenShift Templates, and Helm Charts are all making it possible to define applications at a higher level.\n",
    "\n",
    "Second, not all pieces of software are easy to deploy as containers. \n",
    "\n",
    "> Sometimes, it makes sense to containerize only certain components which are easier to move to containers or provide more value to the overall project. \n",
    "\n",
    "With multi-service application, some services may be deployed as containers, while others may be deployed through a traditional methodology such as an RPM or installer script - see **Pet Containers**. But, other components might be difficult to put into containers because they are too tightly coupled to break up, need access to special hardware, or perhaps require lower level kernel APIs, etc. Within a larger application there will likely be parts of the application that can be containerized, and parts that can't. Containerized components represent the parts that can and are containerized. Containerized components are intended to be ran as part of a specific application, not standalone. It’s important to understand that \n",
    "\n",
    "> containerized components are not designed to function on their own.  \n",
    "\n",
    "They provide value to a larger piece of software, but provide very little value on their own.\n",
    "\n",
    "For example, when OpenShift Enterprise 3.0 was released, most of the core code was deployed using RPMs, but after installation administrators deployed the router and registry as containers. With the release of OpenShift 3.1 an option was added to the installer to deploy the master, node, openvswitch and etcd components as containers - after installation, administrators were given the option to deploy elasticsearch, fluentd, and kibana as containers.\n",
    "\n",
    "While the OpenShift installer still makes modifications to a server’s file system, all of the major software components can now be installed using container images. What makes these containerized components is that, for example, an instance of the etcd image built into OpenShift should and would never be used to store data for your customer facing application code, because it is a containerized component designed to be run as part of OpenShift Container Platform.\n",
    "\n",
    "With the latest releases of OpenShift, there is a trend towards more and more containerized components. The containerized component pattern is becoming more and more common and other software vendors are seeing an advantage to deploying as containerized components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d248db-11c3-46cd-8388-bdfa22c43baf",
   "metadata": {},
   "source": [
    "# <b>Deployer Images</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e83f445-b0fb-4abc-bac6-5853a267bfb1",
   "metadata": {},
   "source": [
    "A **deployer image** is a specialized kind of container which, when run, deploys or manages other containers. This pattern enables sophisticated deployment techniques such as mandating the start order of containers, or first run logic such as populating schema or data.\n",
    "\n",
    "As an example, the “image/container type” pattern is used to deploy the logging and metrics in OpenShift. Deploying these components with a deployer container allows the OpenShift engineering team to control start order of the different components and make sure they are all up and running together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a1cab1-e93b-4c31-8caf-4d7226c2acec",
   "metadata": {},
   "source": [
    "## Intermediate Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7607c1b-281e-4fb0-8d41-07fe88bc57b9",
   "metadata": {},
   "source": [
    "An **Intermediate image** is any container image that relies on a base image. Typically, core builds, middleware and language runtimes are built as layers on “top of” a base image. These images are then referenced in the `FROM` directive of another image. These images are not used on their own, they are typically used as a building block to build a standalone image.\n",
    "\n",
    "It is common to have different teams of specialists own different layers of an image. Systems administrators may own the core build layer, while “developer experience” may own the middleware layer. Intermediate Images are built to be consumed by other teams building images, but can sometimes be run standalone too, especially for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a572801e-57da-4ee1-8b49-1e2332ed938c",
   "metadata": {},
   "source": [
    "## Intermodal Container Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e95ef6-cf4f-446e-a352-aa92a46ccef2",
   "metadata": {},
   "source": [
    "**Intermodal container images**, analogous to [intermodal shipping containers](https://www.marineinsight.com/maritime-history/the-history-of-containerization-in-the-shipping-industry/), are images that have hybrid architectures. For example, many Red Hat Software Collections images can be used in two ways. \n",
    "- First, they can be used as simple Application Containers running a fully contained Ruby on Rails and Apache server. \n",
    "- Second, they can be used as Builder Images inside of OpenShift Container Platform. In this case, the output child images which contain Ruby on Rails, Apache, and the application code which the source to image process was pointed towards during the build phase.\n",
    "\n",
    "The intermodal pattern is becoming more and more common to solve two business problems with one container image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90275337-2a03-47be-9929-b6e840c95cf6",
   "metadata": {},
   "source": [
    "## System Containers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a099d89f-580a-4d8b-befa-c1a2b0c3a017",
   "metadata": {},
   "source": [
    "When system software is distributed as a container, it often needs to run super privileged. To make this deployment easier, and to allow these containers to start before the container runtime or orchestration, Red Hat developed a special container pattern called **System Containers**. \n",
    "\n",
    "System Containers start early in the boot process and rely on the atomic command and systemd to be started independent of any container runtime or orchestration. Red Hat provides System Containers for many pieces of software including rsyslog, cockpit, etcd, and flanneld. In the future, Red Hat will expand the list.\n",
    "\n",
    "This design pattern will make it easier for administrators to add these services to Red Hat Enterprise Linux and Atomic Host in a modular way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e27fcbd-4d65-4e70-905f-5ba2109ff49f",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926874b2-2222-4de2-87cc-6e1b86387b62",
   "metadata": {},
   "source": [
    "Containers are quite easy to consume, but when building a production container environment, it shifts the complexity behind the scenes. To be able to discuss architectures, and how you will build your environment, it's important to have shared nomenclature. There are a lot of pitfalls as you dig deeper into building and architecting your environment. We leave you with a couple of critical ones to remember.\n",
    "\n",
    "People often use the words container image and repository interchangeably and the docker sub-commands don’t make a distinction between an image and a repository. The commands are quite easy to use, but once architecture discussions start, it’s important to understand that a repository is really the central data structure.\n",
    "\n",
    "It’s also quite easy to misunderstand the difference between a namespace, repository, image layer, and tag. Each of these has an architectural purpose. While different vendors, and users are using them for different purposes, they are tools in our toolbox."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv3.12",
   "language": "python",
   "name": "venv3.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
